{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Y&N+W2V+LSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOsXAD9k+3PAGzvh29HyLwM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Qxwjra1zMwRn","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoAFmQESgt2T","colab_type":"code","colab":{}},"source":["!pip install konlpy\n","!pip install kss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVhuHgC-hVRH","colab_type":"code","colab":{}},"source":["!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-3StvQXhdUP","colab_type":"code","colab":{}},"source":["cd Mecab-ko-for-Google-Colab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnT2v3CYhf3_","colab_type":"code","colab":{}},"source":["!bash install_mecab-ko_on_colab190912.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbHA9THA_SIl","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Activation\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder; LE = LabelEncoder()\n","from sklearn.metrics import classification_report\n","from gensim.models.word2vec import Word2Vec\n","from konlpy.tag import Mecab\n","\n","def lstm():\n","    model = Sequential()\n","    model.add(LSTM(2 ** 3, input_shape=(max_len, 1), return_sequences=True))\n","    model.add(LSTM(2 ** 2, return_sequences=True))\n","    model.add(LSTM(2 ** 2, return_sequences=False))\n","\n","    model.add(Dense(2))\n","    model.add(Activation('softmax'))\n","    adam = optimizers.Adam(lr=0.001)\n","    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n","    model.summary()\n","    early_stopping = EarlyStopping(patience=10)\n","    history = model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n","\n","    return model, history\n","\n","if __name__ == \"__main__\":\n","    max_len = 32\n","    epochs = 2 ** 7\n","    batch_size = 2 ** 10\n","\n","    mecab = Mecab()\n","\n","    model = Word2Vec.load('/gdrive/My Drive/한양대학교/2020 프로젝트 학기제/TEST/word2vec')\n","\n","    depression_dataset_path = r'/gdrive/My Drive/한양대학교/2020 프로젝트 학기제/TEST/depression_dataset_맞춤법O.csv'\n","\n","    depression_dataset = pd.read_csv(depression_dataset_path, encoding='cp949')\n","\n","    df = []\n","\n","    for i in range(len(depression_dataset)):\n","        df.append(mecab.morphs(depression_dataset.iloc[i,0]))\n","\n","    for i in range(len(df)):\n","        for j in range(len(df[i])):\n","            try:\n","                df[i][j] = model.wv.get_vector(df[i][j]).tolist()\n","            except:\n","                df[i][j] = False\n","\n","    for i in range(len(df)):\n","        while False in df[i]:\n","            df[i].remove(False)\n","\n","    for i in range(len(df)):\n","        if len(df[i]) >= max_len:\n","            df[i] = df[i][-32:]\n","        else:\n","            for j in range(max_len-len(df[i])):\n","                df[i].insert(0,0)\n","\n","    for i in range(len(df)):\n","        for j in range(len(df[i])):\n","            if df[i][j] == 0:\n","                df[i][j] = [0]*100\n","\n","    depression_dataset['label_1'] = LE.fit_transform(depression_dataset['label_1'])\n","\n","    x_train, x_test, y_train, y_test = train_test_split(df, depression_dataset['label_1'], test_size=0.1, random_state=42)\n","\n","    x_train = np.array(x_train).reshape((len(x_train), max_len*100, 1))\n","    x_test = np.array(x_test).reshape((len(x_test), max_len*100, 1))\n","\n","    y_true = copy.deepcopy(y_test)\n","    y_train = to_categorical(y_train)\n","    y_test = to_categorical(y_test)\n","\n","    print('train_shape : {} / {}'.format(x_train.shape, y_train.shape))\n","    print('test_shape : {} / {}'.format(x_test.shape, y_test.shape))\n","\n","    model, history = lstm()\n","\n","    scores = model.evaluate(x_test, y_test)\n","    print(scores)\n","    print(\"정확도: %.2f%%\" % (scores[1] * 100))\n","\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    epochs = range(1, len(acc) + 1)\n","\n","    plt.plot(epochs, loss, 'r', label='Training loss')\n","    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.figure()\n","    plt.plot(epochs, acc, 'r', label='Training acc')\n","    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.show()\n","\n","    y_true = list(y_true)\n","    y_pred = model.predict_classes(x_test)\n","    y_pred = list(y_pred)\n","\n","    print(classification_report(y_true, y_pred))\n","    print(pd.crosstab(pd.Series(y_true), pd.Series(y_pred), rownames=['True'], colnames=['Predicted']))"],"execution_count":null,"outputs":[]}]}